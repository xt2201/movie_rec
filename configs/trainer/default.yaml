# Default Trainer Configuration
name: default

# Training
max_epochs: 300
learning_rate: 0.005  # Paper uses 0.005
weight_decay: 0.0  # Set to 0, use BPR reg_weight instead
batch_size: ${data.dataloader.batch_size}

# Optimizer
optimizer:
  name: adam  # adam, adamw, sgd
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Learning rate scheduler
scheduler:
  name: reduce_on_plateau  # reduce_on_plateau, cosine, step, none
  mode: min
  factor: 0.5
  patience: 5
  min_lr: 1.0e-6

# Early stopping
early_stopping:
  enabled: true
  patience: 20
  min_delta: 0.0001
  monitor: val_loss
  mode: min

# Gradient clipping
gradient_clip:
  enabled: true
  max_norm: 1.0

# Checkpointing
checkpoint:
  enabled: true
  save_top_k: 3
  monitor: val_loss
  mode: min
  save_last: true

# Validation
val_check_interval: 1  # Validate every N epochs
eval_every_n_epochs: 5  # Run full ranking evaluation (ndcg, recall, etc.) every N epochs

# Mixed precision training
mixed_precision: false

# Debugging
detect_anomaly: false
